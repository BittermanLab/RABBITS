Model,Original,g2b,Average,Difference
GPT-3.5-turbo-0125,97.29,96.86,97.08,-0.42
GPT-4o,90.36,87.42,88.89,-2.94
GPT4-0613,92.00,89.37,90.69,-2.63
Gemini 1 Pro,69.36,73.44,71.40,4.07
Gemini 1.5 Flash,97.25,95.43,96.34,-1.82
Gemini 1.5 Pro,87.56,84.89,86.22,-2.67
Llama-2-70B-hf,53.66,49.31,51.49,-4.35
Llama-2-7b-hf,35.75,34.15,34.95,-1.60
Llama-3-70B,76.64,69.71,73.18,-6.93
Llama-3-8B,60.02,53.95,56.99,-6.08
Mistral-7B-v0.3,55.03,48.48,51.76,-6.55
Mixtral-8x22B-v0.1,70.92,64.62,67.77,-6.29
Mixtral-8x7B-v0.1,63.69,57.76,60.72,-5.93
Phi-3-medium-4k,65.44,56.89,61.16,-8.55
Qwen2-72B,76.64,72.81,74.72,-3.83
Qwen2-7B,61.25,54.44,57.84,-6.82
Yi-1.5-34B,66.90,59.78,63.34,-7.12
aya-23-35B,51.97,48.22,50.09,-3.75
claude-3-opus@20240229,86.10,81.61,83.85,-4.49
command-r-plus,60.91,52.88,56.89,-8.03
phi-1,23.38,22.91,23.15,-0.47
phi-2,43.08,39.72,41.40,-3.36
phi1.5,32.56,33.27,32.91,0.70